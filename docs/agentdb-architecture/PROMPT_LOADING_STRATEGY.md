# ğŸ“ Prompt Loading Strategie: Filesystem vs RAM vs Anthropic Cache

## ğŸ¯ Die Frage: Wann mÃ¼ssen Prompt-Dateien geladen werden?

### âœ… OPTIMALE STRATEGIE: 3-Ebenen Caching

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EBENE 1: RAM CACHE (In-Memory)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Lifetime: Bis Server-Neustart oder manuelle Invalidierung     â”‚
â”‚  Speed: <1ms                                                    â”‚
â”‚  Cost: Kostenlos                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ (bei RAM-Miss)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EBENE 2: FILESYSTEM (Prompt Dateien)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Lifetime: Permanent (bis manuell geÃ¤ndert)                     â”‚
â”‚  Speed: ~10ms                                                   â”‚
â”‚  Cost: Kostenlos                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ (immer)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EBENE 3: ANTHROPIC PROMPT CACHE                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Lifetime: 5 Minuten nach letzter Nutzung                       â”‚
â”‚  Speed: Normale LLM-Speed (aber 90% billiger!)                  â”‚
â”‚  Cost: $0.30 per 1M tokens (statt $3.00)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âŒ FALSCHE Annahme:

> "Alle 5 Minuten muss ich die Prompt-Dateien vom Filesystem neu laden"

**NEIN!** Anthropic's Cache-Ablauf bedeutet NICHT, dass Sie die Dateien neu laden mÃ¼ssen!

---

## âœ… RICHTIGE Strategie:

```typescript
class PromptManager {
  // RAM CACHE - Bleibt im Memory!
  private ontologyPrompt: string | null = null;
  private agentRoles: Map<string, string> = new Map();
  private lastLoaded: Date | null = null;

  /**
   * LÃ¤dt Prompts vom Filesystem â†’ RAM
   * NUR beim Server-Start oder bei manueller Invalidierung!
   */
  async initialize() {
    console.log('Loading prompts from filesystem...');

    // EINMAL vom Disk laden
    this.ontologyPrompt = await fs.readFile('./prompts/ontology.md', 'utf-8');

    this.agentRoles.set(
      'requirements-specialist',
      await fs.readFile('./prompts/agents/requirements.md', 'utf-8')
    );

    this.agentRoles.set(
      'architecture-designer',
      await fs.readFile('./prompts/agents/architecture.md', 'utf-8')
    );

    this.lastLoaded = new Date();
    console.log('âœ… Prompts loaded into RAM');
  }

  /**
   * Gibt Prompt aus RAM zurÃ¼ck
   * KEINE Filesystem-Operation!
   */
  getOntologyPrompt(): string {
    if (!this.ontologyPrompt) {
      throw new Error('Prompts not initialized!');
    }
    return this.ontologyPrompt; // â† Aus RAM!
  }

  /**
   * Gibt Agent-Prompt aus RAM zurÃ¼ck
   */
  getAgentPrompt(agentType: string): string {
    const prompt = this.agentRoles.get(agentType);
    if (!prompt) {
      throw new Error(`Agent prompt not found: ${agentType}`);
    }
    return prompt; // â† Aus RAM!
  }

  /**
   * Invalidiert RAM-Cache (z.B. nach Prompt-Ã„nderungen)
   * NUR bei manuellen Ã„nderungen nÃ¶tig!
   */
  async reload() {
    console.log('Reloading prompts from filesystem...');
    this.ontologyPrompt = null;
    this.agentRoles.clear();
    await this.initialize();
    console.log('âœ… Prompts reloaded');
  }

  /**
   * Hot-Reload bei Datei-Ã„nderungen (optional)
   */
  watchForChanges() {
    fs.watch('./prompts', { recursive: true }, async (event, filename) => {
      console.log(`Prompt file changed: ${filename}`);
      await this.reload();
    });
  }
}

// Singleton
export const promptManager = new PromptManager();
```

---

## ğŸ”¥ Der Flow im Detail:

### Server Start:

```typescript
// server.ts
import { promptManager } from './prompt-manager';

async function startServer() {
  // 1. Prompts EINMAL beim Start laden
  await promptManager.initialize(); // â† Filesystem â†’ RAM

  console.log('âœ… Prompts loaded into RAM');
  console.log('Server ready!');
}

startServer();
```

**Filesystem-Zugriffe: 1x beim Start** âœ…

---

### Request-Handling (100+ Queries):

```typescript
// ai-assistant.service.ts

async function handleQuery(userMessage: string) {
  // RAM â†’ Anthropic Cache
  const systemPrompt = [
    {
      type: 'text',
      text: promptManager.getOntologyPrompt(), // â† Aus RAM! (keine Filesystem-IO)
      cache_control: { type: 'ephemeral' }
    }
  ];

  // Anthropic API Call
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    system: systemPrompt, // â† Gecacht bei Anthropic fÃ¼r 5 Min
    messages: [{ role: 'user', content: userMessage }]
  });

  return response;
}

// 100 Queries = 0 Filesystem-Zugriffe! âœ…
```

**Filesystem-Zugriffe: 0** (alles aus RAM!) âœ…

---

## â±ï¸ Timing-Diagramm (100 Queries in 10 Minuten):

```
Zeit    | Filesystem | RAM Cache | Anthropic Cache | Kosten
â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€
0:00    | âœ… Load    | âœ… Hit    | Miss (first)    | $3.00
0:01    |            | âœ… Hit    | âœ… Hit (cached) | $0.30
0:02    |            | âœ… Hit    | âœ… Hit (cached) | $0.30
0:03    |            | âœ… Hit    | âœ… Hit (cached) | $0.30
0:04    |            | âœ… Hit    | âœ… Hit (cached) | $0.30
0:05    |            | âœ… Hit    | âœ… Hit (cached) | $0.30
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        (5 Min Anthropic Cache lÃ¤uft ab wegen InaktivitÃ¤t)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0:06    |            | âœ… Hit    | Miss (expired)  | $3.00
0:07    |            | âœ… Hit    | âœ… Hit (cached) | $0.30
0:08    |            | âœ… Hit    | âœ… Hit (cached) | $0.30
...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Filesystem-Zugriffe total: 1 (nur beim Start)
RAM Cache Hits:           100 (jede Query!)
Anthropic Cache:          90% der Queries gecacht
```

---

## ğŸ¯ Die 3 Cache-Ebenen im Code:

```typescript
class AIAssistantService {
  private promptManager: PromptManager;

  constructor() {
    this.promptManager = promptManager; // â† Singleton (RAM Cache)
  }

  async chat(userMessage: string) {
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // EBENE 1: RAM CACHE (promptManager)
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const ontologyPrompt = this.promptManager.getOntologyPrompt(); // â† RAM!
    const agentPrompt = this.promptManager.getAgentPrompt('requirements'); // â† RAM!

    // Keine Filesystem-Operationen! âœ…

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // EBENE 2: agentDB (Semantic Cache)
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const cached = await agentdb.vectorSearch({ query: userMessage });
    if (cached.similarity > 0.85) {
      return cached.operations; // â† Skip LLM komplett!
    }

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // EBENE 3: ANTHROPIC PROMPT CACHE
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const systemPrompt = [
      {
        type: 'text',
        text: ontologyPrompt, // â† Aus RAM!
        cache_control: { type: 'ephemeral' } // â† Anthropic cached 5 Min
      }
    ];

    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      system: systemPrompt,
      messages: [{ role: 'user', content: userMessage }]
    });

    return response;
  }
}
```

---

## ğŸ“Š Performance-Vergleich:

### âŒ SCHLECHTE Strategie (Filesystem bei jeder Query):

```typescript
async function badApproach(userMessage: string) {
  // JEDES MAL vom Disk lesen! âŒ
  const ontology = await fs.readFile('./prompts/ontology.md', 'utf-8'); // 10ms

  const response = await anthropic.messages.create({
    system: [{ text: ontology, cache_control: { type: 'ephemeral' } }],
    messages: [{ role: 'user', content: userMessage }]
  });

  return response;
}

// 100 Queries = 100 Ã— 10ms = 1000ms Filesystem-Overhead! âŒ
```

### âœ… GUTE Strategie (RAM Cache):

```typescript
async function goodApproach(userMessage: string) {
  // Aus RAM lesen! âœ…
  const ontology = promptManager.getOntologyPrompt(); // <1ms

  const response = await anthropic.messages.create({
    system: [{ text: ontology, cache_control: { type: 'ephemeral' } }],
    messages: [{ role: 'user', content: userMessage }]
  });

  return response;
}

// 100 Queries = 100 Ã— <1ms = <100ms RAM-Overhead! âœ…
```

**Speedup: 10x schneller!** ğŸš€

---

## ğŸ”„ Wann muss man Prompts neu laden?

### âœ… Szenarien fÃ¼r Reload:

```typescript
// 1. Server-Start (automatisch)
await promptManager.initialize();

// 2. Manuelle Prompt-Ã„nderungen (Development)
app.post('/admin/reload-prompts', async (req, res) => {
  await promptManager.reload();
  res.json({ success: true, message: 'Prompts reloaded' });
});

// 3. Hot-Reload (optional, fÃ¼r Development)
if (process.env.NODE_ENV === 'development') {
  promptManager.watchForChanges(); // â† Auto-Reload bei Ã„nderungen
}

// 4. Scheduled Reload (optional, z.B. tÃ¤glich)
cron.schedule('0 0 * * *', async () => {
  await promptManager.reload(); // â† TÃ¤glich um Mitternacht
});
```

### âŒ NICHT nÃ¶tig:

- âŒ Nach Anthropic Cache-Ablauf (5 Min)
- âŒ Bei jeder Query
- âŒ Periodisch ohne Grund

---

## ğŸ’¡ Best Practices:

```typescript
class OptimizedPromptManager {
  private cache: Map<string, { content: string; loadedAt: Date }> = new Map();

  /**
   * Lazy Loading - nur bei Bedarf vom Disk
   */
  async getPrompt(key: string): Promise<string> {
    // 1. Check RAM Cache
    const cached = this.cache.get(key);
    if (cached) {
      return cached.content; // â† RAM Hit!
    }

    // 2. Load from Filesystem (nur bei Cache-Miss)
    const content = await fs.readFile(`./prompts/${key}.md`, 'utf-8');

    // 3. Store in RAM
    this.cache.set(key, {
      content,
      loadedAt: new Date()
    });

    return content;
  }

  /**
   * Selective Invalidation - nur geÃ¤nderte Prompts
   */
  invalidate(key: string) {
    this.cache.delete(key);
    console.log(`Invalidated prompt: ${key}`);
  }

  /**
   * Smart Reload - nur wenn wirklich nÃ¶tig
   */
  async reloadIfModified(key: string) {
    const cached = this.cache.get(key);
    if (!cached) return;

    const stats = await fs.stat(`./prompts/${key}.md`);
    const fileModified = stats.mtime;

    if (fileModified > cached.loadedAt) {
      console.log(`File modified: ${key}, reloading...`);
      this.invalidate(key);
      await this.getPrompt(key); // â† LÃ¤dt neu
    }
  }
}
```

---

## ğŸ¯ Zusammenfassung: Wann wird was geladen?

| Zeitpunkt | Filesystem | RAM Cache | Anthropic Cache | Grund |
|-----------|------------|-----------|-----------------|-------|
| **Server Start** | âœ… Load | âœ… Fill | - | Initialisierung |
| **Query 1** | - | âœ… Hit | Miss | Erster Call |
| **Query 2-100** | - | âœ… Hit | âœ… Hit | Normalbetrieb |
| **Nach 5 Min Pause** | - | âœ… Hit | Miss | Cache expired |
| **Query 101** | - | âœ… Hit | Miss | Neuer Cache-Cycle |
| **Prompt geÃ¤ndert** | âœ… Reload | â™»ï¸ Update | - | Manual/Hot-Reload |
| **Server Neustart** | âœ… Load | âœ… Fill | - | RAM verloren |

---

## âœ… Finale Code-Struktur:

```typescript
// startup.ts
import { promptManager } from './prompt-manager';

async function bootstrap() {
  console.log('ğŸš€ Starting server...');

  // 1. Prompts laden (EINMAL)
  await promptManager.initialize();
  console.log('âœ… Prompts loaded into RAM');

  // 2. Hot-Reload in Development
  if (process.env.NODE_ENV === 'development') {
    promptManager.watchForChanges();
    console.log('ğŸ‘€ Watching for prompt changes...');
  }

  // 3. Server starten
  app.listen(3000);
  console.log('âœ… Server running on port 3000');
}

bootstrap();
```

```typescript
// ai-assistant.service.ts
export class AIAssistantService {
  async chat(userMessage: string) {
    // RAM â†’ Anthropic (kein Filesystem!)
    const systemPrompt = [
      {
        type: 'text',
        text: promptManager.getOntologyPrompt(), // â† RAM!
        cache_control: { type: 'ephemeral' }     // â† Anthropic cached 5 Min
      }
    ];

    const response = await anthropic.messages.create({
      system: systemPrompt,
      messages: [{ role: 'user', content: userMessage }]
    });

    return response;
  }
}
```

---

## ğŸš€ Performance-Metriken:

**Production (1000 Queries/Tag):**

| Metrik | Mit Filesystem-Load pro Query | Mit RAM Cache |
|--------|-------------------------------|---------------|
| Filesystem-Zugriffe | 1000/Tag | 1/Tag (beim Start) |
| Latenz-Overhead | ~10ms/Query | <1ms/Query |
| I/O-Wait | 10 Sekunden/Tag | <1ms/Tag |
| **Speedup** | Baseline | **10,000x schneller!** ğŸš€ |

---

## âœ… Fazit:

**NEIN - Sie mÃ¼ssen Prompts NICHT alle 5 Minuten neu laden!**

1. âœ… **Beim Server-Start**: Einmal vom Filesystem â†’ RAM
2. âœ… **Bei Queries**: Aus RAM â†’ Anthropic Cache
3. âœ… **Bei Ã„nderungen**: Manuell/Hot-Reload
4. âŒ **NICHT**: Nach Anthropic Cache-Ablauf

**Anthropic Cache-Ablauf â‰  RAM Cache-Ablauf!**

Der RAM-Cache bleibt bis Server-Neustart oder manuelle Invalidierung! ğŸ‰
