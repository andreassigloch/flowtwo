# ğŸ”„ Caching-Strategie: Anthropic Prompt Cache vs agentDB vs Agent Prompt Cache

## ğŸ¯ Die 3 Caching-Ebenen (KOMPLEMENTÃ„R!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER QUERY                                  â”‚
â”‚  "Erstelle Use Cases fÃ¼r Authentifizierung"                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EBENE 1: agentDB KNOWLEDGE CACHE (Semantic Cache)                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Purpose: Skip LLM entirely for known answers                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                     â”‚
â”‚  Query agentDB: "Authentifizierung Use Cases"                      â”‚
â”‚  â†“                                                                  â”‚
â”‚  Similarity Search: 0.96 match found!                               â”‚
â”‚  â†“                                                                  â”‚
â”‚  âœ… RETURN CACHED OPERATIONS                                       â”‚
â”‚  âŒ SKIP LLM CALL COMPLETELY!                                      â”‚
â”‚                                                                     â”‚
â”‚  Savings: 100% of LLM cost (no API call!)                          â”‚
â”‚  Speed: <50ms (database query only)                                â”‚
â”‚  Applies to: Similar questions (semantic matching)                 â”‚
â”‚                                                                     â”‚
â”‚  Example:                                                           â”‚
â”‚  - "Auth use cases" â†’ Match!                                       â”‚
â”‚  - "Login functionality" â†’ Match! (0.88 similarity)                â”‚
â”‚  - "User authentication flows" â†’ Match! (0.91 similarity)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ No match (similarity < 0.85)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EBENE 2: ANTHROPIC PROMPT CACHING (System Prompt Cache)           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Purpose: Reduce token cost for repeated system prompts            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                     â”‚
â”‚  System Prompt Structure:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ # Systems Engineering Assistant                          â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## Ontology Rules (1500 tokens) â† CACHED! âœ…            â”‚    â”‚
â”‚  â”‚ - SYS must have at least one ACTOR                       â”‚    â”‚
â”‚  â”‚ - UC requires INTERACTS relationship                     â”‚    â”‚
â”‚  â”‚ - FUNC decomposes UC                                     â”‚    â”‚
â”‚  â”‚ ... (full ontology)                                      â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## Current Neo4j State (500 tokens) â† CACHED! âœ…         â”‚    â”‚
â”‚  â”‚ - 5 Systems, 12 Actors, 23 Use Cases                     â”‚    â”‚
â”‚  â”‚ - Graph structure...                                     â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## agentDB Episodes (800 tokens) â† CACHED! âœ…            â”‚    â”‚
â”‚  â”‚ - Episode #42: "Auth UC creation"                        â”‚    â”‚
â”‚  â”‚ - Episode #17: "Security requirements"                   â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## User Query (50 tokens) â† NOT CACHED (unique)          â”‚    â”‚
â”‚  â”‚ "Erstelle Use Cases fÃ¼r Authentifizierung"              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  Token Pricing:                                                     â”‚
â”‚  - Normal: $3.00 per 1M input tokens                               â”‚
â”‚  - Cached: $0.30 per 1M input tokens (90% cheaper!)                â”‚
â”‚                                                                     â”‚
â”‚  Cost Calculation:                                                  â”‚
â”‚  WITHOUT Prompt Cache:                                              â”‚
â”‚    (1500 + 500 + 800 + 50) Ã— $3.00 = $0.00855                      â”‚
â”‚                                                                     â”‚
â”‚  WITH Prompt Cache:                                                 â”‚
â”‚    Cached: (1500 + 500 + 800) Ã— $0.30 = $0.00084                   â”‚
â”‚    Fresh:  (50) Ã— $3.00 = $0.00015                                 â”‚
â”‚    Total: $0.00099                                                 â”‚
â”‚                                                                     â”‚
â”‚  Savings: 88.4% per query!                                          â”‚
â”‚  Speed: ~same (cache hit is fast, but LLM still runs)              â”‚
â”‚  Applies to: Identical system prompt blocks                        â”‚
â”‚                                                                     â”‚
â”‚  âš ï¸ WICHTIG: LLM wird TROTZDEM aufgerufen!                        â”‚
â”‚              Nur die Tokens sind billiger!                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                      LLM Processes Query
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                            â”‚
    Simple Task                               Complex Task
        â”‚                                            â”‚
        â–¼                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EBENE 3: AGENT PROMPT CACHE (fÃ¼r Specialized Agents)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Purpose: Reduce token cost for specialized agent prompts          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                     â”‚
â”‚  Agent Prompt Structure (Requirements Specialist):                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ # Requirements Engineering Specialist                    â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## Role & Methodology (2000 tokens) â† CACHED! âœ…         â”‚    â”‚
â”‚  â”‚ You are an expert in Requirements Engineering...         â”‚    â”‚
â”‚  â”‚ - IEEE 29148 standards                                   â”‚    â”‚
â”‚  â”‚ - SMART requirements patterns                            â”‚    â”‚
â”‚  â”‚ - Traceability best practices                            â”‚    â”‚
â”‚  â”‚ ... (full methodology)                                   â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## SE Ontology Context (1500 tokens) â† CACHED! âœ…        â”‚    â”‚
â”‚  â”‚ - REQ connects to FUNC via SATISFIES                     â”‚    â”‚
â”‚  â”‚ - TEST validates REQ via VALIDATES                       â”‚    â”‚
â”‚  â”‚ ... (ontology rules)                                     â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## agentDB Knowledge (1200 tokens) â† CACHED! âœ…          â”‚    â”‚
â”‚  â”‚ - 10 similar episodes                                    â”‚    â”‚
â”‚  â”‚ - 5 proven skills                                        â”‚    â”‚
â”‚  â”‚ - 3 causal edges                                         â”‚    â”‚
â”‚  â”‚                                                           â”‚    â”‚
â”‚  â”‚ ## Specific Task (100 tokens) â† NOT CACHED               â”‚    â”‚
â”‚  â”‚ "Analyze authentication requirements for..."            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  Cost WITHOUT Agent Prompt Cache:                                  â”‚
â”‚    (2000 + 1500 + 1200 + 100) Ã— $3.00 = $0.0144                    â”‚
â”‚                                                                     â”‚
â”‚  Cost WITH Agent Prompt Cache:                                     â”‚
â”‚    Cached: (2000 + 1500 + 1200) Ã— $0.30 = $0.00141                 â”‚
â”‚    Fresh:  (100) Ã— $3.00 = $0.00030                                â”‚
â”‚    Total: $0.00171                                                 â”‚
â”‚                                                                     â”‚
â”‚  Savings: 88.1% per agent call!                                    â”‚
â”‚                                                                     â”‚
â”‚  âš ï¸ WICHTIG: Nutzt DIESELBE Anthropic Prompt Cache API!           â”‚
â”‚              Kein separater Cache nÃ¶tig!                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Klare Antwort: Braucht es 3 separate Caches?

### âŒ NEIN - Kein separater "Agent Prompt Cache" nÃ¶tig!

**Warum?**
- Anthropic Prompt Caching funktioniert **automatisch** fÃ¼r ALLE Prompts
- Egal ob Master LLM oder Agent - beide nutzen dieselbe Cache API
- Einfach statische Prompt-Teile markieren mit Cache-Breakpoints

---

## ğŸ“Š Die 3 Ebenen im Vergleich

| Ebene | Was wird gecacht? | Wie funktioniert's? | Savings | Braucht separate Implementierung? |
|-------|-------------------|---------------------|---------|-----------------------------------|
| **1. agentDB** | Komplette Antworten (Operations) | Semantic Vector Search | 100% (kein LLM!) | âœ… JA - Custom DB |
| **2. Anthropic Prompt Cache** | System Prompts (Master LLM) | Anthropic API Feature | 90% Token-Cost | âŒ NEIN - Built-in |
| **3. Agent Prompts** | Agent-spezifische Prompts | **SELBE** Anthropic API | 90% Token-Cost | âŒ NEIN - Built-in |

---

## ğŸ’¡ Wichtige Unterschiede

### agentDB Cache (Ebene 1):
```typescript
// Semantic matching - ÃœBERSPRINGT LLM komplett!
const cached = await agentdb.vectorSearch({
  query: "authentication use cases",
  threshold: 0.85
});

if (cached.similarity > 0.85) {
  return cached.operations; // âœ… Keine LLM API Call!
}
```

**Eigenschaften:**
- âœ… Semantische Suche (Ã¤hnliche != identische Queries)
- âœ… 100% Savings (kein LLM Call)
- âœ… Schnellste Option (<50ms)
- âŒ Nur fÃ¼r bereits gelÃ¶ste Probleme

---

### Anthropic Prompt Cache (Ebene 2 & 3):
```typescript
// Beide nutzen DIESELBE API - kein separater Cache!
const response = await anthropic.messages.create({
  model: "claude-3-5-sonnet-20241022",
  system: [
    {
      type: "text",
      text: "# Ontology Rules...", // 1500 tokens
      cache_control: { type: "ephemeral" } // â† Cache-Breakpoint!
    },
    {
      type: "text",
      text: "# Current State...", // 500 tokens
      cache_control: { type: "ephemeral" } // â† Cache-Breakpoint!
    },
    {
      type: "text",
      text: "# agentDB Episodes..." // 800 tokens
      cache_control: { type: "ephemeral" } // â† Cache-Breakpoint!
    }
  ],
  messages: [
    { role: "user", content: "Erstelle Use Cases..." } // â† NICHT gecacht
  ]
});
```

**Eigenschaften:**
- âœ… Exakte Text-BlÃ¶cke werden gecacht
- âœ… 90% Token-Savings fÃ¼r gecachte BlÃ¶cke
- âš ï¸ LLM wird TROTZDEM aufgerufen (nur billiger!)
- âš ï¸ Cache verfÃ¤llt nach 5 Minuten InaktivitÃ¤t
- âœ… Funktioniert fÃ¼r Master LLM UND Agents (gleiche API!)

---

## ğŸš€ Optimale Strategie: ALLE 3 EBENEN kombinieren!

### Beispiel-Flow mit allen 3 Caches:

```typescript
async function handleUserQuery(userMessage: string) {
  // 1ï¸âƒ£ EBENE 1: agentDB Check (Semantic Cache)
  const knownSolution = await agentdb.vectorSearch({
    query: userMessage,
    threshold: 0.85
  });

  if (knownSolution.similarity > 0.85) {
    console.log("âœ… Cache HIT - agentDB (100% savings, 0ms LLM)");
    return knownSolution.operations; // â† Kein LLM Call!
  }

  // 2ï¸âƒ£ EBENE 2: Anthropic Prompt Cache (Master LLM)
  const systemPrompt = [
    {
      type: "text",
      text: getOntologyRules(), // 1500 tokens
      cache_control: { type: "ephemeral" } // â† Gecacht!
    },
    {
      type: "text",
      text: await getNeo4jState(), // 500 tokens
      cache_control: { type: "ephemeral" } // â† Gecacht!
    },
    {
      type: "text",
      text: await getAgentDBEpisodes(userMessage), // 800 tokens
      cache_control: { type: "ephemeral" } // â† Gecacht!
    }
  ];

  const masterResponse = await anthropic.messages.create({
    model: "claude-3-5-sonnet-20241022",
    system: systemPrompt, // â† 88% billiger!
    messages: [{ role: "user", content: userMessage }]
  });

  console.log("âš¡ Prompt Cache savings: ~88%");

  // Complexity check
  if (masterResponse.needsSpecialist) {
    // 3ï¸âƒ£ EBENE 3: Agent mit SELBER Prompt Cache API
    const agentPrompt = [
      {
        type: "text",
        text: getAgentRole("requirements-specialist"), // 2000 tokens
        cache_control: { type: "ephemeral" } // â† Gecacht!
      },
      {
        type: "text",
        text: getOntologyRules(), // 1500 tokens (bereits gecacht oben!)
        cache_control: { type: "ephemeral" }
      },
      {
        type: "text",
        text: await loadAgentDBContext(), // 1200 tokens
        cache_control: { type: "ephemeral" } // â† Gecacht!
      }
    ];

    const agentResponse = await anthropic.messages.create({
      model: "claude-3-5-sonnet-20241022",
      system: agentPrompt, // â† Auch 88% billiger!
      messages: [{ role: "user", content: masterResponse.agentTask }]
    });

    console.log("âš¡ Agent Prompt Cache savings: ~88%");

    // Store result in agentDB for future
    await agentdb.store(userMessage, agentResponse);
  }

  // Store in agentDB fÃ¼r zukÃ¼nftige Queries
  await agentdb.store(userMessage, masterResponse);

  return masterResponse.operations;
}
```

---

## ğŸ“Š Kosten-Vergleich bei 100 Queries

### Szenario: 50 Unique, 50 Repeated

#### Ohne jegliches Caching:
```
100 queries Ã— 2850 tokens Ã— $3.00 = $0.855
```

#### Nur Anthropic Prompt Cache:
```
100 queries Ã— (2800 cached @ $0.30 + 50 fresh @ $3.00) = $0.099
Savings: 88.4%
```

#### Nur agentDB Cache:
```
50 unique Ã— 2850 tokens Ã— $3.00 = $0.4275
50 repeated = $0 (no LLM!)
Savings: 50%
```

#### BEIDE kombiniert (optimal!):
```
50 unique Ã— (2800 cached @ $0.30 + 50 fresh @ $3.00) = $0.0495
50 repeated = $0 (agentDB cache!)
Total: $0.0495
Savings: 94.2%! ğŸ‰
```

---

## âœ… Fazit: Was brauchen Sie wirklich?

### JA - Implementieren:
1. âœ… **agentDB** - Custom Knowledge Cache (separates System)
2. âœ… **Anthropic Prompt Caching** - FÃ¼r Master LLM (built-in)
3. âœ… **Dieselbe Prompt Cache** - FÃ¼r Agents (automatisch!)

### NEIN - NICHT nÃ¶tig:
âŒ Separater "Agent Prompt Cache" (nutzt Anthropic's Cache!)
âŒ Dritte Caching-LÃ¶sung
âŒ Redis/Memcached fÃ¼r Prompts (Anthropic macht das!)

---

## ğŸ¯ Implementation Checklist

```typescript
// âœ… 1. agentDB fÃ¼r Knowledge Cache
const agentdb = new AgentDBService();

// âœ… 2. Anthropic Prompt Cache fÃ¼r Master LLM
const masterPrompt = [
  { text: ontology, cache_control: { type: "ephemeral" } },
  { text: state, cache_control: { type: "ephemeral" } }
];

// âœ… 3. SELBE API fÃ¼r Agent Prompts - kein Extra-Code!
const agentPrompt = [
  { text: agentRole, cache_control: { type: "ephemeral" } },
  { text: ontology, cache_control: { type: "ephemeral" } } // â† Wieder verwendet!
];
```

**Keine separaten Systeme nÃ¶tig - Anthropic's Cache ist universal!** ğŸš€

---

## ğŸ’° ROI Calculation (Realistisch)

**Production System (1 Monat):**
- 10,000 Queries
- 40% repeated (agentDB cache hits)
- 60% unique (Prompt cache savings)

**Kosten:**
- Ohne Caching: $285
- Mit nur Prompt Cache: $33
- Mit nur agentDB: $171
- **Mit BEIDEN: $19.80**

**Savings: $265/Monat (93% Reduktion!)** ğŸ‰

Die Investition in agentDB lohnt sich zusÃ¤tzlich zu Anthropic's Built-in Cache!
