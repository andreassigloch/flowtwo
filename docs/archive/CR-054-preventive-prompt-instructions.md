# CR-054: PrÃ¤ventive Prompt-Instruktionen & Validation Feedback

**Type:** Feature
**Status:** Completed
**Priority:** CRITICAL
**Created:** 2024-12-15
**Depends on:** -
**Blocks:** CR-055

## Problem / Use Case

LLM erzeugt neue Fehler beim Reparieren von Graphen:
1. **Keine prÃ¤ventiven Instruktionen** - Prompt fehlt "CRITICAL ERRORS TO AVOID" wie in AiSE
2. **Validierungsfeedback nicht an LLM** - /analyze, Background Validation nur geloggt, nicht in chatHistory
3. **Episode Success immer TRUE** - ReflexionMemory lernt nichts aus Fehlern

## Requirements

### Functional Requirements
- FR-054.1: Prompt enthÃ¤lt "CRITICAL ERRORS TO AVOID" Section (generiert aus ontology-rules.json)
- FR-054.2: /analyze Ergebnis erscheint im LLM-Kontext (chatHistory)
- FR-054.3: Background Validation Violations erscheinen in chatHistory
- FR-054.4: Episode success basiert auf Reward-Threshold, nicht hardcoded `true`

### Non-Functional Requirements
- NFR-054.1: Prompt-Generierung aus ontology-rules.json (keine hardcoded Texte)
- NFR-054.2: Bestehende Tests passieren weiterhin

## Success Criteria

**PrimÃ¤re Akzeptanzkriterien (messbar):**
1. **Keine neuen Fehler im LLM Output** - LLM darf keine neuen Violations einfÃ¼hren
2. **Nach Fixes weniger Fehler als vorher** - Violation Count muss sinken

**Technische Kriterien:**
3. Prompt enthÃ¤lt "CRITICAL ERRORS TO AVOID" (generiert)
4. /analyze Ergebnis in chatHistory sichtbar
5. Episode success = (reward >= 0.7)
6. Bestehende Tests passieren

## Architecture / Solution Approach

### Teil 1: PrÃ¤ventive Prompt-Instruktionen (Phase 0)

**ontology-rules.json erweitern:**
```json
"llmContext": {
  "criticalErrors": [
    {
      "rule": "io_bidirectional_flow",
      "emoji": "âš ï¸",
      "title": "BIDIRECTIONAL IO EDGES",
      "wrong": "Node -io-> FLOW AND FLOW -io-> same Node",
      "right": "A -io-> FLOW -io-> B (unidirectional chain)"
    },
    {
      "rule": "io_circular_chain",
      "emoji": "âš ï¸",
      "title": "CIRCULAR IO CHAINS",
      "wrong": "FUNC -io-> FLOW -io-> same FUNC",
      "right": "Linear chain without cycles"
    }
    // ... weitere aus integrityRules mit type: "hard"
  ]
}
```

**PromptBuilder erweitern:**
- Lese `ontology-rules.json`
- Generiere "CRITICAL ERRORS TO AVOID" Section aus `criticalErrors`
- Generiere "Pre-Submission Validation" Checklist

### Teil 2: ChatHistory Feedback (Phase 3)

**validation-commands.ts:**
```typescript
// Nach console output
const feedbackMsg = this.formatValidationForChat(result);
ctx.chatCanvas.addSystemMessage(feedbackMsg);
```

**background-validator.ts:**
```typescript
if (violations.length > 0) {
  const feedback = generateFeedbackForLLM(violations);
  chatCanvas.addSystemMessage(`ðŸ” Validation: ${feedback}`);
}
```

### Teil 3: Episode Success Bug Fix

**session-manager.ts:656:**
```typescript
const successThreshold = 0.7;
const success = reward >= successThreshold;

await this.agentDB.storeEpisode(
  agent,
  task,
  success,  // Basiert auf Reward!
  { response, operations },
  success ? undefined : `Failed: reward ${reward.toFixed(2)} < ${successThreshold}`
);
```

**Gleiches Fix in chat-interface.ts:439 und :460**

## Implementation Plan

### Phase 1: Episode Success Bug (5min)
- Fix session-manager.ts:656
- Fix chat-interface.ts:439
- Fix chat-interface.ts:460

### Phase 2: ontology-rules.json erweitern (15min)
- Add `llmContext.criticalErrors` array
- Map integrityRules mit `type: "hard"` zu criticalErrors

### Phase 3: PromptBuilder erweitern (30min)
- Lade ontology-rules.json
- Generiere "CRITICAL ERRORS TO AVOID" Section
- Generiere "Pre-Submission Validation" Checklist

### Phase 4: ChatHistory Feedback (30min)
- Modify validation-commands.ts
- Modify background-validator.ts

## Files to Modify

| File | Changes |
|------|---------|
| `src/session-manager.ts` | Episode success basierend auf reward |
| `src/terminal-ui/chat-interface.ts` | Episode success basierend auf reward |
| `settings/ontology-rules.json` | Add llmContext.criticalErrors |
| `src/llm-engine/prompt-builder.ts` | Generate CRITICAL ERRORS section |
| `src/terminal-ui/commands/validation-commands.ts` | Add chatHistory feedback |
| `src/llm-engine/validation/background-validator.ts` | Add chatHistory feedback |

## Estimated Effort

| Task | Time |
|------|------|
| Episode Success Bug | 5min |
| ontology-rules.json | 15min |
| PromptBuilder | 30min |
| ChatHistory Feedback | 30min |
| **Total** | **1h 20min** |

## Current Status

**Completed: 2024-12-15**

### Implemented Changes

1. **Episode Success Bug Fixed** âœ…
   - `session-manager.ts:653-667`: Episode success now based on `reward >= 0.7`
   - `chat-interface.ts:438-455`: Same fix applied

2. **ontology-rules.json Extended** âœ…
   - Added `llmContext.criticalErrors` array with 5 critical error patterns
   - Added `llmContext.preSubmissionChecklist` with 5 validation steps

3. **PromptBuilder Extended** âœ…
   - `prompt-builder.ts`: Added `loadOntologyRules()`, `getCriticalErrorsSection()`, `getPreSubmissionChecklist()`
   - Generates "CRITICAL ERRORS TO AVOID" section from ontology-rules.json
   - Generates "Pre-Submission Validation" checklist

4. **ChatHistory Feedback** âœ…
   - `validation-commands.ts:177-269`: `/analyze` results added to chatHistory via `ctx.chatCanvas.addSystemMessage()`

### Test Results
- Build: âœ… Passes
- TypeScript: âœ… No errors
- Unit tests: âœ… prompt-builder tests pass (9/9)
- Integration: Some pre-existing infrastructure issues (WebSocket port conflicts) - not related to CR-054

## References

- Plan: /Users/andreas/.claude/plans/transient-sparking-bubble.md
- AiSE Prompts: ./aise/prompts/3.FuncDecomp/system.md (Vorbild fÃ¼r CRITICAL ERRORS)
- ontology-rules.json: settings/ontology-rules.json
